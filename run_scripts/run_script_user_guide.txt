==========================
GDA2020 ROUTINE PROCESSING
==========================

+ This document displays the commands executed in each component of the GDA2020 routine processing. 
  It is designed to be used by a user when conducting any part of the processing. Bash/CLI commands 
  are identified by the '>>>' marker where they can be copy and pasted into the terminal.

+ The routine processing is made up of 4 seperate procedures:
	1. UPDATE THE APREF CUMULATIVE SOLUTION.
	2. NGCA PROCESSING.
	3. JADJ QA/QC
	4. NADJ PROCESSING

+ Glossery of terms:
	APREF - Australia-Pacific Reginal Reference Frame (in this context is is the cumulative solution).
	NGCA  - National GNSS Campaign Archive. 
	JADJ  - Jurisdictional Adjustment. 
	NADJ  - National Adjustment. 
	JDA   - Jurisdictional Data Archive. 


====================================
UPDATE THE APREF CUMULATIVE SOLUTION
====================================

Schedule:	Run APREF cumulative solution post-processing whenever a new solution is available (every ~8 weeks).
Machine:        gda2020 (National Geodesy).

+ The aim here is to post-process the APREF cumulative solution
  to generate GDA2020 constraints for the national adjustment, and
  NGCA processing. 
  
+ The post-processing refers to these key procedures:

	1. Exclude stations outside of the GDA2020 bounds. 
	
	2. Perform a two-step coordinate transformation:
	
		I)  ITRF2020@2015.0 --> ITRF2014@2015.0
		II) ITRF2014@2015.0 --> ITRF2014@2020.0 (GDA2020)
		
	3. Add type-B uncertainties:
		- 3,3,6 mm for RVS stations.
		- 6,6,12 mm for non-RVS stations.
		
	4. Split the SINEX file into constraint and non-constraint SINEX files.
		- Stations with total duration < 2 years classified as non-constraint.
		
	5. Remove velocities from the constraint SINEX file: 
		- This is only to reduce the file size. 
		- Because the national adjustment does not need velocities. 
		
------
STEP 1 - Download APREF cumulative solution files
------

+ Identify if a there is a new APREF cumulative solution available. 
+ Identify if that solution is > 3 months old. 

	# On GDA2020 machine:
	
		>>> cd ~/apref/workDir/
		>>> aws s3 ls s3://gnss-analysis/results/combination/combinations_IGS20/
		>>> aws s3 sync s3://gnss-analysis/results/combination/combinations_IGS20/YYYYMMDD/ ./ 

------	
STEP 2 - Post-process the APREF cumulative solution
------
+ run_APREF_STEP_2.sh.
+ Recently has taken ~21 hours (mostly in sinex2epoch).
+ This part of the process relies on two existing files in /apref/workDir/.
	1. gda2020.dat.
	2. RVS_GDA2020.txt.
	
	# On GDA2020 machine:
	
		>>> cd /home/fedora/run_scripts/
		>>> vim run_APREF_STEP_2.sh 
		
		NOW: edit the APREF solution date at top of script.
	
		>>> [ESC] :wq	
		>>> nohup ./run_APREF_STEP_2.sh &

------	
STEP 3 - Upload post-processed APREF
------
	
	# On GDA2020 machine: If exists, delete the old version in /home/fedora/apref/ 
	
		>>> cd /home/fedora/apref/
		>>> rm apref* disconts*
	
	# On GDA2020 machine: Setup the new apref version
	
		>>> cd /home/fedora/apref/YYYYMMDD
		>>> cp apref* disconts* ../
		>>> cd ../
		>>> rm apref*.VEL
		>>> aws s3 ls s3://gda2020-ngca/adjustmentData/
		>>> aws s3 rm s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "apref*"
		>>> aws s3 rm s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "disconts*"
		>>> aws s3 cp /home/fedora/apref/ s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "apref*"
		>>> aws s3 cp /home/fedora/apref/ s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "disconts*"
	
	NOW: email the AWG
	

------	
STEP 4 - Copy post-processed APREF TO NADJ machine
------

	# On NADJ machine: Delete the old apref version
	
		>>> cd /home/fedora/Data/
		>>> rm apref* disconts* 
		
		This should be:
			- aprefYYYYMMDD.disconts
			- aprefYYYYMMDD_msr.xml 
			- aprefYYYYMMDD.rvs 
			- aprefYYYYMMDD.snx 
			- aprefYYYYMMDD_stn.xml 
			- aprefYYYYMMDD.txt 
			- aprefRename_msr.xml 
			- aprefRename_stn.xml 
			- discontsYYYYMMDD.snx
	
	
	# On NADJ machine: Download the new apref version
		
		>>> aws s3 cp s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "apref*"
		>>> aws s3 cp s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "disconts*"		
	
	# On NADJ machine:
	
		>>> dnaimport -n aprefRename_ aprefYYYYMMDD.snx --discontinuity-file discontsYYYYMMDD.snx --export-xml
		>>> rm aprefRename_.*
		>>> mkdir apref/YYYYMMDD 
		>>> cp aprefYYYYMMDD* discontsYYYYMMDD.snx aprefRename* apref/YYYYMMDD 

		NOW: turn the NADJ machine off.


===============
NGCA PROCESSING 
=============== 

Schedule: 	Run NGCA processing on the 15th of every 2nd month.
Machines: 	gda2020 (National Geodesy) and geodesy (GNSS Analysis). 

------
STEP 1 - Download the archives
------

+ Download the latest NGCAs from GA s3 bucket via SFTP.
+ This is done by running the "get_ngca.py" script from any directory on the gda2020 machine.
+ The script will download NGCAs from each jurisdiction, unless the "-j" flag is provided 
	- e.g. "get_ngca.py -j tas" will download only the Tasmanian NGCA. 

	# On gda2020 machine:
	
		>>> get_ngca.py
	
		OR
	
		>>> get_ngca.py -j <jurisdiction>
	
+ Ask Carl to increase the amount of available ec2s for AUSPOS (usually upto 300 or 600).
+ Currently have to give Carl our tempprary IP adress for the gda2020 machine.
	- Note: should change our gda2020 machine to have a static IP. 
	- Note: should change method of connection to session manager.  
	
------	
STEP 2 - Preprocessing
------

Run script: 	run_NGCA_PRE_PROCESSING.sh

+ The "run_NGCA_PRE_PROCESSING.sh" script runs all jurisdictions NGCA steps up until before AUSPOS step.
+ It is necessary to set the NGCA archive date within the script. 
+ It is also possible to change which jurisdictions are run.
	
	# On GDA2020 machine
	
		>>> cd /home/fedora/run_scripts/
		>>> vim run_NGCA_PRE_PROCESSING.sh 
		
		NOW: edit the NGCA archive date at top of script.
		
		>>> [ESC] :wq	
		>>> nohup ./run_NGCA_PRE_PROCESSING.sh &

------
STEP 3 - AUSPOS Processing
------

+ This sequence of steps is done one jurisdiction at a time. 
+ Access to the AUSPOS machine is required (geodesy@[IP ADDRESS]).
	- From gda2020 machine, run this command to log onto the AUSPOS/geodesy machine:
		>>> ssh -i ~/.ssh/ga_ngca geodesy@[IP ADDRESS]
	- All work happens in the "/data/craig/" directory.

--- 3.1: Copy single jurisdiction archive to AUSPOS machine

	# On GDA2020 machine:
	
		>>> cd /home/fedora/ngca/[JURIS]
		>>> scp -i ~/.ssh/ga_ngca -r $archive geodesy@[IP ADDRESS]:/data/craig/ 

	# On AUSPOS/Geodesy machine:
	
		>>> cd /data/craig/
	
	NOW: Confirm GDA2020 mail box is clean before starting each jurisdiction processing.

--- 3.2: Submit RINEX files to AUSPOS	

	# On AUSPOS/Geodesy machine:
	
		>> nohup ./GDA2020_submit_multiJobs_SNX.sh $archive &
	
	NOW: monitor emails to see when processing is complete

--- 3.3: Download SINEX files from AUSPOS

	# On AUSPOS/Geodesy machine:
	
		>>> bash down_4_SNX.sh
		>>> ls SNX | wc -l 
		>>> mv log_4_submit.txt down_4_SNX.sh nohup.out $archive
		>>> cd $archive
		>>> ../get_solutions.py
		>>> ls solutions/*.SNX | wc -l 
		>>> ls solutions/*_ls | wc -l
	
--- 3.4 Transfer AUSPOS solutions from AUSPOS/Geodesy machine to the gda2020 machine	
	
	# On GDA2020:
	
		>>> cd /home/fedora/run_scripts/
		>>> vim run_NGCA_DOWNLOAD_FROM_AUSPOS.sh
		
		NOW: - edit the NGCA archive date at top of script.
	 	     - edit the jurisdiction at top of script.
		
		>>> [ESC] :wq	
		>>> ./run_NGCA_DOWNLOAD_FROM_AUSPOS.sh

	# Do some sanity checks
		
		>>> ls ../ngca/[$juris]/sinexFiles/ | wc -l
		>>> ls ../ngca/[$juris]/rinexantls/ | wc -l
	
	# On AUSPOS/Geodesy machine:
	
		>>> cd ..
		>>> rm -r $archive SNX job tmp [job_check]

------
STEP 4 - Post-processing
------

Run script: 	run_NGCA_POST_PROCESSING.sh

+ Needs DynaML.xsd in /home/fedora/.

	# On GDA2020 machine:
	
		>>> cd /home/fedora/RUN-SCRIPTS/
		>>> vim run_NGCA_POST_PROCESSING.sh 
		
		NOW: 	- edit the NGCA archive date at top of script.
			- edit the jurisdiction list to account for which jurisdiction(s) you plan to do.
				
		>>> [ESC] :wq	
		>>> nohup ./run_NGCA_POST_PROCESSING.sh &
	
	
------
STEP 5 - Finish NGCA
------

	# On GDA2020 machine:
	
		>>> cd /home/fedora/ngca/sent/
		>>> ls -lotr
	
		NOTE: 
			* the archive(s) will be the last file(s) listed.
			* worth checking the size of the .zip file for basic check. 

	NOW: email jurisiction(s). 
	NOW: if all NGCA is complete, turn off gda2020 machine.
	

=========
JADJ QAQC - Windows Version (a temporary solution)
=========

Schedule: 	Run JADJ QA/QC when/if a jurisdiction supplies it between NGCA and next NADJ.
Machines: 	NADJ Windows (National Geodesy) / NADJ Linux (when NADJ issue is solved). 

+ This section covers the checks done on the JADJ. 
+ It is to be done for only those jurisdictions that submit a JADJ. 
+ It occurs between NGCA and the NADJ. 
+ It is for the Windows machine (NADJ Windows), a temporary solution whilst it 
  is investigated how to run the national adjustment on Linux again.
+ The JADJ QA/QC refers to these steps:
	1. Duplicate station check.
	2. Near station check.
	3. 1-iteration adjustment
+ Jurisdictional adjustment data are only excluded from the national adjustment if they 
  cause the 1-iteration to fail dramatically. It is hard to define what failure is here. 
  For example, sometimes large station shifts occur, but they are ligitamite because they 
  are caused by improved data included by the jurisdiction. It is best to check with the 
  jurisdiction if unsure. Another definition of failure could be to check the sigma-0 value. 
  If it is > 50 than something could be wrong. Fifty is an uninformed guess for a quality 
  sigma-0 threshold. I have not done tests to identify what threshold for a single-iteration 
  sigma-0 value would make sense.
+ Steps in AWS console to connect to Windows machine: 

	> Systems Manager > Fleet Manager > Node Actions > Connect > Connect with Remote Desktop > Key pair

	# On NADJ Windows machine: 

	+ Download the jurisdictional adjustment data:

		>>> cd \Data
		>>> aws s3 cp s3://gda2020-ngca/adjustmentData/ stn/ --recursive --exclude "*" --include "[JUR]*.adj.xml"
		>>> aws s3 cp s3://gda2020-ngca/adjustmentData/ msr/ --recursive --exclude "*" --include "[JUR]*_msr.xml"

	+ Move the old files to the staging area:

		>>> cd stn
		>>> dir
		>>> move [JUR]_GDA2020_YYYYMMDD.adj.xml staging 

		>>> cd ..\msr
		>>> dir
		>>> move [JUR]_GDA2020_YYYYMMDD_msr.xml staging
		>>> cd ..

	+ Download auxiliary files:

		>>> aws s3 cp s3://gda2020-ngca/files/ renaming/ --recursive --exclude "*" --include "[jur]*.renaming"
		>>> aws s3 cp s3://gda2020-ngca/files/ renaming/ --recursive --exclude "*" --include "[jur]*.ignore"
		>>> aws s3 cp s3://gda2020-ngca/files/ nearStns/ --recursive --exclude "*" --include "[jur]*.near"
	
		NOTE: Move the old versions of these files into the staging folder of each respective directory (renaming, nearStns)
                      if they exist. 
	
	+ Run the duplicate station search:

		>>> python runNatAdjust.py -s dup
		>>> perl checkDST.pl

		NOTE: - Check the .dup file for identified duplicates, i.e. '>>> type gda2020_YYYYMMDD.dup.dup'.
		      - Notify jurisdictions involved to fix/clarify respective duplicates.
		      	* I have not been enforcing this lately due to development in this process.

		>>> del gda2020_YYYYMMDD.dup*

	+ Run the near station search:

		>>> python runNatAdjust.py -s near
		>>> move gda2020_YYYYMMDD.near.dst nearStns
		>>> del gda2020_YYYYMMDD.near.bat
		>>> cd nearStns
		>>> perl filterNearStns.pl 

		NOTE: - Check the .dst file for identified near stations, i.e. '>>> type gda2020_YYYYMMDD.near.dst'.
		      - Notify jurisdictions involved to fix/clarify respective near stations.
			* I have not been enforcing this lately due to development in this process.

		>>> del gda2020* notUsedIgnore.dat
		>>> cd ..\

	+ Run the 1-iteration adjustment (~9hrs):

		>>> python runNatAdjust.py -qa

		NOTE: - ...



===============
NADJ PROCESSING - !NEED TO UPDATE THIS! | !IN DEVELOPMENT!
===============

Schedule: 	Run NADJ on 1st of every 2nd month.
Machines: 	NADJ Windows (National Geodesy) / NADJ Linux (when NADJ issue is solved). 

	+ Run as normal
	
		# Run the national adjustment (and wait for it to finish):
			
			>>> nohup runNatAdjust.pl &
		
			NOTE: When finished, run the following commands for post-processing and packaging of NADJ:
			
		# Organise, and add Type-B uncertainties:
			
			>>> mv nohup.out adjustments/gda2020_YYYYMMDD 
			>>> cd adjustments/gda2020_YYYYMMDD
			>>> addTypeB.py
			
		# Organise:

			>>> cd ..
			>>> cp -r gda2020_YYYYMMDD sent/ 
			>>> zip -r gda2020_YYYYMMDD.zip gda2020_YYYYMMDD/
			>>> rm -r gda2020_YYYYMMDD/
			>>> cd sent/gda2020_YYYYMMDD/
			>>> cp ../gda2020_[PREVIOUS]/gda2020_[PREVIOUS].phased-stage.xyz ./
			
		# Calculate difference between adjustments:
		- Order of solutions doesn't matter.
			
			>>> dynaDiff.py gda2020_YYYYMMDD.phased-stage.xyz gda2020_YYYYMMDD.phased-stage.xyz
			
		# Organsise:

			>>> rm *.sh *.asl *.aml *.d *.dbid *.dnaproj *.map *.data *.rft *.seg
			>>> mkdir inputFiles
			>>> cp ~/AUSGeoid2020_20180201.gsb ~/DynaML.xsd ~/apref20220326* ~/disconts20220326.snx ~/stn/*.xml ~/msr/*.xml inputFiles/ 
			>>> cd ..
			>>> zip -r gda2020_YYYYMMDD.zip gda2020_YYYYMMDD/
			
		# Upload to FTP, and email AWG:

			>>> ftp ftp.ga.gov.au
			>>> cd adjustments/
			>>> del gda2020_YYYYMMDD.zip 	(delete the oldest adjustment)
			>>> put gda2020_YYYYMMDD.zip    (upload the newest adjustment)
			
			NOW: email AWG using the template email. 
					+ The station and measurement info comes from the .imp file.
					+ everything else comes from the .adj.nstat file.
					
			---- DONE ----

. 
