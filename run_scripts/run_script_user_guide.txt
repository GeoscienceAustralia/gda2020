==========================
GDA2020 ROUTINE PROCESSING
==========================

+ This document displays the commands executed in each component of the GDA2020 routine processing. It 
  is designed to be followed by a user when conducting any part of the processing. Bash/CLI commands 
  are identified by the '>>>' marker where they can be copy and pasted into the terminal.

+ The routine processing is made up of 4 seperate procedures:
	1. UPDATE THE APREF CUMULATIVE SOLUTION.
	2. NGCA PROCESSING.
	3. JADJ QA/QC.
	4. NADJ PROCESSING.

+ Glossery of terms:
	APREF - Australia-Pacific Reginal Reference Frame (in this context it is reference to the cumulative solution).
	NGCA  - National GNSS Campaign Archive. 
	JADJ  - Jurisdictional Adjustment. 
	NADJ  - National Adjustment. 
	JDA   - Jurisdictional Data Archive. 


====================================
UPDATE THE APREF CUMULATIVE SOLUTION
====================================

Schedule:	Run APREF cumulative solution post-processing whenever a new solution is available (every ~8 weeks).
Machine:        gda2020 (National Geodesy).

+ The aim here is to post-process the APREF cumulative solution
  to generate GDA2020 constraints for the national adjustment, and
  NGCA processing. 
  
+ The post-processing refers to these key procedures:

	1. Exclude stations outside of the GDA2020 bounds. 
	
	2. Perform a two-step coordinate transformation:
	
		I)  ITRF2020@2015.0 --> ITRF2014@2015.0
		II) ITRF2014@2015.0 --> ITRF2014@2020.0 (GDA2020)
		
	3. Add type-B uncertainties:
		- 3,3,6 mm for RVS stations.
		- 6,6,12 mm for non-RVS stations.
		
	4. Split the SINEX file into constraint and non-constraint SINEX files.
		- Stations with total duration < 2 years classified as non-constraint.
		
	5. Remove velocities from the constraint SINEX file: 
		- This is only to reduce the file size. 
		- Because the national adjustment does not need velocities. 
		
------
STEP 1 - Download APREF cumulative solution files
------

+ Identify if a there is a new APREF cumulative solution available. 
+ Identify if that solution is > 3 months old. 

	# On GDA2020 machine:
	
		>>> cd ~/apref/workDir/
		>>> aws s3 ls s3://gnss-analysis/results/combination/combinations_IGS20/
		>>> aws s3 cp s3://gnss-analysis/results/combination/combinations_IGS20/YYYYMMDD/AUS0OPSSNX_YYYYDOY_YYYYDOY_00U_SOL.SNX.gz ./
		>>> aws s3 cp s3://gnss-analysis/results/combination/combinations_IGS20/YYYYMMDD/AUS0OPSSNX_YYYYDOY_YYYYDOY_00U_DSC.SNX ./

------	
STEP 2 - Post-process the APREF cumulative solution
------
+ run_APREF_STEP_2.sh.
+ Recently has taken ~21 hours (mostly in sinex2epoch).
+ This part of the process relies on two existing files in /apref/workDir/.
	1. gda2020.dat.
	2. RVS_GDA2020.txt.
	
	# On GDA2020 machine:
	
		>>> cd ~/run_scripts/
		>>> vim run_APREF_STEP_2.sh 
		
		NOW: edit the APREF solution date at top of script.
	
		>>> [ESC] :wq	
		>>> nohup ./run_APREF_STEP_2.sh &

------	
STEP 3 - Upload post-processed APREF
------
	
	# On GDA2020 machine: If exists, delete the old version in ~/apref/ 
	
		>>> cd ~/apref/
		>>> rm apref* disconts*
	
	# On GDA2020 machine: Setup the new apref version
	
		>>> cd ~/apref/YYYYMMDD
		>>> cp apref* disconts* ../
		>>> cd ../
		>>> rm apref*.VEL
		>>> aws s3 ls s3://gda2020-ngca/adjustmentData/
		>>> aws s3 rm s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "apref*"
		>>> aws s3 rm s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "disconts*"
		>>> aws s3 cp ~/apref/ s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "apref*"
		>>> aws s3 cp ~/apref/ s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "disconts*"
	
	NOW: email the AWG
	

------	
STEP 4 - Copy post-processed APREF TO NADJ machine
------

	# On NADJ machine: Delete the old apref version
	
		>>> cd ~/Data/
		>>> rm apref* disconts* 
		
		This should be:
			- aprefYYYYMMDD.disconts
			- aprefYYYYMMDD_msr.xml 
			- aprefYYYYMMDD.rvs 
			- aprefYYYYMMDD.snx 
			- aprefYYYYMMDD_stn.xml 
			- aprefYYYYMMDD.txt 
			- aprefRename_msr.xml 
			- aprefRename_stn.xml 
			- discontsYYYYMMDD.snx
	
	
	# On NADJ machine: Download the new apref version
		
		>>> aws s3 cp s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "apref*"
		>>> aws s3 cp s3://gda2020-ngca/adjustmentData/ --recursive --exclude "*" --include "disconts*"		
	
	# On NADJ machine:
	
		>>> dnaimport -n aprefRename_ aprefYYYYMMDD.snx --discontinuity-file discontsYYYYMMDD.snx --export-xml
		>>> rm aprefRename_.*
		>>> mkdir apref/YYYYMMDD 
		>>> cp aprefYYYYMMDD* discontsYYYYMMDD.snx aprefRename* apref/YYYYMMDD 

		NOW: turn the NADJ machine off.


===============
NGCA PROCESSING 
=============== 

Schedule: 	Run NGCA processing on the 15th of every 2nd month.
Machines: 	gda2020 (National Geodesy) and geodesy (GNSS Analysis). 

------
STEP 1 - Download the archives
------

+ Download the latest NGCAs from GA s3 bucket via SFTP.
+ This is done by running the "get_ngca.py" script from any directory on the gda2020 machine.
+ The script will download NGCAs from each jurisdiction, unless the "-j" flag is provided 
	- e.g. "get_ngca.py -j tas" will download only the Tasmanian NGCA. 

	# On gda2020 machine:
	
		>>> get_ngca.py
	
		OR
	
		>>> get_ngca.py -j <jurisdiction>
	
+ Ask Carl to increase the amount of available ec2s for AUSPOS (usually upto 300 or 600).
+ Currently have to give Carl our tempprary IP adress for the gda2020 machine.
	- Note: should change our gda2020 machine to have a static IP. [Done]
    - Note: We have static IP for GDA2020 machine [2025].
	- Note: should change method of connection to session manager.  
	
------	
STEP 2 - Preprocessing
------

Run script: 	run_NGCA_PRE_PROCESSING.sh

+ The "run_NGCA_PRE_PROCESSING.sh" script runs all jurisdictions NGCA steps up until before AUSPOS step.
+ It is necessary to set the NGCA archive date within the script. 
+ It is also possible to change which jurisdictions are run.
	
	# On GDA2020 machine
	
		>>> cd ~/run_scripts/
		>>> vim run_NGCA_PRE_PROCESSING.sh 
		
		NOW: edit the NGCA archive date at top of script.
		
		>>> [ESC] :wq	
		>>> nohup ./run_NGCA_PRE_PROCESSING.sh &

------
STEP 3 - AUSPOS Processing
------

+ This sequence of steps is done one jurisdiction at a time. 
+ Access to the AUSPOS machine is required (geodesy@[IP ADDRESS]).
	- From gda2020 machine, run this command to log onto the AUSPOS/geodesy machine:
		>>> ssh -i ~/.ssh/ga_ngca geodesy@[IP ADDRESS]
	- All work happens in the "/data/craig/" directory.

--- 3.1: Copy single jurisdiction archive to AUSPOS machine

	# On GDA2020 machine:
	
		>>> cd ~/ngca/[JURIS]
		>>> scp -i ~/.ssh/ga_ngca -r $archive geodesy@[IP ADDRESS]:/data/craig/ 

	# On AUSPOS/Geodesy machine:
	
		>>> cd /data/craig/
	
	NOW: Confirm GDA2020 mail box is clean before starting each jurisdiction processing.

--- 3.2: Submit RINEX files to AUSPOS	

	# On AUSPOS/Geodesy machine:
	
		>> nohup ./GDA2020_submit_multiJobs_SNX.sh $archive &
	
	NOW: monitor emails to see when processing is complete

--- 3.3: Download SINEX files from AUSPOS

	# On AUSPOS/Geodesy machine:
	
		>>> bash down_4_SNX.sh
		>>> ls SNX | wc -l 
		>>> mv log_4_submit.txt down_4_SNX.sh nohup.out $archive
		>>> cd $archive
		>>> ../get_solutions.py
		>>> ls solutions/*.SNX | wc -l 
		>>> ls solutions/*_ls | wc -l
	
--- 3.4 Transfer AUSPOS solutions from AUSPOS/Geodesy machine to the gda2020 machine	
	
	# On GDA2020:
	
		>>> cd ~/run_scripts/
		>>> vim run_NGCA_DOWNLOAD_FROM_AUSPOS.sh
		
		NOW: - edit the NGCA archive date at top of script.
	 	     - edit the jurisdiction at top of script.
		
		>>> [ESC] :wq	
		>>> ./run_NGCA_DOWNLOAD_FROM_AUSPOS.sh

	# Do some sanity checks
		
		>>> ls ../ngca/[$juris]/sinexFiles/ | wc -l
		>>> ls ../ngca/[$juris]/rinexantls/ | wc -l
	
	# On AUSPOS/Geodesy machine:
	
		>>> cd ..
		>>> rm -r $archive SNX job tmp [job_check]

------
STEP 4 - Post-processing
------

Run script: 	run_NGCA_POST_PROCESSING.sh

+ Needs DynaML.xsd in ~/.

	# On GDA2020 machine:
	
		>>> cd ~/RUN-SCRIPTS/
		>>> vim run_NGCA_POST_PROCESSING.sh 
		
		NOW: 	- edit the NGCA archive date at top of script.
			- edit the jurisdiction list to account for which jurisdiction(s) you plan to do.
				
		>>> [ESC] :wq	
		>>> nohup ./run_NGCA_POST_PROCESSING.sh &
	
	
------
STEP 5 - Finish NGCA
------

	# On GDA2020 machine:
	
		>>> cd ~/ngca/sent/
		>>> ls -lotr
	
		NOTE: 
			* the archive(s) will be the last file(s) listed.
			* worth checking the size of the .zip file for basic check. 
    Note: We have portal that is planned to be automated upto here A lot of testing is required.
	Action: email jurisiction(s). 
	Action: After NGCA completion, turn off gda2020 machine.
	

=========
JADJ QAQC - Windows Version (a temporary solution)
=========

Schedule: 	Run JADJ QA/QC when/if a jurisdiction supplies it between NGCA and next NADJ.
Machines: 	NADJ Windows (National Geodesy) / NADJ Linux (when NADJ issue is solved). 

+ This section covers the checks done on the JADJ. 
+ It is to be done for only those jurisdictions that submit a JADJ. 
+ It occurs between NGCA and the NADJ. 
+ It is for the Windows machine (NADJ Windows), a temporary solution whilst it 
  is investigated how to run the national adjustment on Linux again.
+ The JADJ QA/QC refers to these steps:
	1. Duplicate station check.
	2. Near station check.
	3. 1-iteration adjustment
+ Jurisdictional adjustment data are only excluded from the national adjustment if they 
  cause the 1-iteration to fail dramatically. It is hard to define what failure is here. 
  For example, sometimes large station shifts occur, but they are ligitamite because they 
  are caused by improved data included by the jurisdiction. It is best to check with the 
  jurisdiction if unsure. Another definition of failure could be to check the sigma-0 value. 
  If it is > 50 than something could be wrong. Fifty is an uninformed guess for a quality 
  sigma-0 threshold. I have not done tests to identify what threshold for a single-iteration 
  sigma-0 value would make sense.
- Action: Need to review JADJ QA/QC
+ Steps in AWS console to connect to Windows machine: 

	> Systems Manager > Fleet Manager > Node Actions > Connect > Connect with Remote Desktop > Key pair

	# On NADJ Windows machine: 

	+ Download the jurisdictional adjustment data:

		>>> cd \Data
		>>> aws s3 cp s3://gda2020-ngca/adjustmentData/ stn/ --recursive --exclude "*" --include "[JUR]*.adj.xml"
		>>> aws s3 cp s3://gda2020-ngca/adjustmentData/ msr/ --recursive --exclude "*" --include "[JUR]*_msr.xml"

	+ Move the old files to the staging area:

		>>> cd stn
		>>> dir
		>>> move [JUR]_GDA2020_YYYYMMDD.adj.xml staging 

		>>> cd ..\msr
		>>> dir
		>>> move [JUR]_GDA2020_YYYYMMDD_msr.xml staging
		>>> cd ..

	+ Download auxiliary files:

		>>> aws s3 cp s3://gda2020-ngca/files/ renaming/ --recursive --exclude "*" --include "[jur]*.renaming"
		>>> aws s3 cp s3://gda2020-ngca/files/ renaming/ --recursive --exclude "*" --include "[jur]*.ignore"
		>>> aws s3 cp s3://gda2020-ngca/files/ nearStns/ --recursive --exclude "*" --include "[jur]*.near"
	
		NOTE: Move the old versions of these files into the staging folder of each respective directory (renaming, nearStns)
                      if they exist. 
	
	+ Run the duplicate station search:

		>>> python runNatAdjust.py -s dup
		>>> perl checkDST.pl

		NOTE: - Check the .dup file for identified duplicates, i.e. '>>> type gda2020_YYYYMMDD.dup.dup'.
		      - Notify jurisdictions involved to fix/clarify respective duplicates.
		      	* I have not been enforcing this lately due to development in this process.

		>>> del gda2020_YYYYMMDD.dup*

	+ Run the near station search:

		>>> python runNatAdjust.py -s near
		>>> move gda2020_YYYYMMDD.near.dst nearStns
		>>> del gda2020_YYYYMMDD.near.bat
		>>> cd nearStns
		>>> perl filterNearStns.pl 

		NOTE: - Check the .dst file for identified near stations, i.e. '>>> type gda2020_YYYYMMDD.near.dst'.
		      - Notify jurisdictions involved to fix/clarify respective near stations.
			* I have not been enforcing this lately due to development in this process.

		>>> del gda2020* notUsedIgnore.dat
		>>> cd ..\

	+ Temporary Step:

		NOTE: - Need to develop way to automatically identify sites with both orthometric and ellipsoid heights, so
			automatically assign the 'Ignore' flag to the orthometric measurements. 
		      - Until this is done, we need to manually assign the 'Ignore' flag to these two already identified 
 			orthometric height measurements in the SA measurement file (SA_GDA2020_YYYYMMMDD_msr.xml). 
		      - Do this manually by using Textpad (or something else), i.e. change '<Ignore/>' to '<Ignore>*</Ignore>'
		        for these two sites:
						267300640
						313200220
						
	+ Run the 1-iteration adjustment (~9hrs):

		>>> python runNatAdjust.py -qa

		NOTE: - Investigate the sigma-0 and maximum correction value to identify potential issues. 
		      - Large corrections or sigma-0 values do not necessarily mean the JADJ needs to be rejected. 
		      - e.g. some jurisdictions are improving old height measurements, leading to ~5 metre corrections
                        in the vertical. These are improvements, but best to confirm with the jurisdictions. 

	+ Move the output files to a temporary folder (JADJ-RESULTS):

		>>> move gda2020_YYYYMMDD.phased-stage.adj JADJ-RESULTS\[JUR]_gda2020_YYYYMMDD.phased-stage.adj
		>>> move gda2020_YYYYMMDD.phased-stage.adj.stn.xml JADJ-RESULTS\[JUR]_gda2020_YYYYMMDD.phased-stage.adj.stn.xml

		NOTE: - Whichever JADJ was processed last, and successfully, it can be used to update the a priori coordinate file 
                        for the national adjustment. 
		      - i.e. the [JUR]_gda2020_YYYYMMDD.phased-stage.adj.stn.xml will become the apriori.xml file.
		      - This will save an iteration in the national adjustment. 
		      - If this is the case, run this command:

			>>> copy JADJ-RESULT\[JUR]_gda2020_YYYYMMDD.phased-stage.adj.stn.xml apriori.xml



===============
NADJ PROCESSING - !NEED TO UPDATE THIS! | !IN DEVELOPMENT!
===============

Schedule: 	Run NADJ on 1st of every 2nd month.
Machines: 	NADJ Windows (National Geodesy) / NADJ Linux (when NADJ issue is solved). 

+ This section covers the steps to run a national adjustment.
+ It is for the Windows machine (NADJ Windows), a temporary solution whilst it 
  is investigated how to run the national adjustment on Linux again.
+ The NADJ processing refers to these steps:
	1. A final duplicate station search (only to keep the output files for posterity).
	2. A final near station search (only to keep the output files for posterity). 
	3. National adjustment. 
	4. Add Type-B uncertainties. 
	5. Package up files to upload to s3.
+ Steps in AWS console to connect to Windows machine: 

	> Systems Manager > Fleet Manager > Node Actions > Connect > Connect with Remote Desktop > Key pair

	# On NADJ Machine:

	+ Open the command prompt, move to working directory:

		>>> cd \Data

	+ Run the duplicate station search:

		>>> python runNatAdjust.py -s dup
		>>> perl checkDST.pl
		>>> move gda2020_YYYYMMDD.dup.dup searches\
		>>> del gda2020_YYYYMMDD.dup*
	
	+ Run the near station search:

		>>> python runNatAdjust.py -s near
		>>> move gda2020_YYYYMMDD.near.dst nearStns
		>>> del gda2020_YYYYMMDD.near.bat
		>>> cd nearStns
		>>> perl filterNearStns.pl
		>>> move gda2020_YYYYMMDD.near.dst ..\searches
		>>> del gda2020_YYYYMMDD.near.dst.bak notUsedIgnore.dat
		>>> cd ..

	+ Run the national adjustment (~9.5 hours/iteration)

		>>> python runNatAdjust.py

		NOTE: - Usually 2-3 iterations (~20-30 total hours of processing time). 
		      - When finished, run the next steps for organising files and post-processing. 

	+ Add Type-B uncertainties to the adjusted coordinates

		>>> copy addTypeB.py adjustments\gda2020_YYYYMMDD
		>>> cd adjustments\gda2020_YYYYMMDD
		>>> python addTypeB.py
		>>> del addTypeB.py
		>>> cd ..
		>>> mkdir sent\gda2020_YYYYMMDD
		>>> copy gda2020_YYYYMMDD\* sent\gda2020_YYYYMMDD
		>>> cd ..

	+ Create adjustment comparison

		>>> copy dynaDiff.py adjustments\sent\gda2020_YYYYMMDD
		>>> copy big_ntstats.py adjustments\sent\gda2020_YYYYMMDD
		>>> cd adjustments\sent\gda2020_YYYYMMDD
		>>> copy ..\gda2020_[PREVIOUS_start]\gda2020_[PREVIOUS_end].phased-stage.xyz .\
		>>> python dynaDiff.py gda2020_YYYYMMDD.phased-stage.xyz gda2020_YYYYMMDD.phased-stage.xyz
		>>> del *.sh *.asl *.aml *.d *.dbid *.dnaproj *.map *.data *.rft *.seg
    + creates a .csv file
		
	+ Identify largest N-statistics

		>>> python big_nstats.py gda2020_YYYYMMDD.phased-stage.adj
		>>> del big_nstats.py dynaDiff.py

	+ Prepare folder

		>>> mkdir inputFiles
		>>> cd ..\..\..\
		>>> copy AUSGeoid2020_20180201.gsb adjustments\sent\gda2020_YYYYMMDD\inputFiles
		>>> copy DynaMl.xsd adjustments\sent\gda2020_YYYYMMDD\inputFiles
		>>> copy aprefYYYMMDD* adjustments\sent\gda2020_YYYYMMDD\inputFiles
		>>> copy disconts*.snx adjustments\sent\gda2020_YYYYMMDD\inputFiles
		>>> copy stn\*.xml adjustments\sent\gda2020_YYYYMMDD\inputFiles
		>>> copy msr\*.xml adjustments\sent\gda2020_YYYYMMDD\inputFiles
		>>> cd adjustments\sent
		>>> tar -a -c -f gda2020_YYYYMMDD.zip gda2020_YYYYMMDD

	+ Upload to s3

		>>> aws s3 cp gda2020_YYYYMMDD.zip s3://gda2020-ngca/adjustments/

		NOTE: - Can think about a schedule here to arhicve the older ones. For example, could keep
			the latest ten adjustements there, and move the older ones to long-term storage.
		      - Email the AWG. 
		

	
--- END OF DOCUMENT ---

